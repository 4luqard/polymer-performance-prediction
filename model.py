{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nBaseline Ridge Regression Model for NeurIPS Open Polymer Prediction 2025\nFinal version for Kaggle submission - no internet dependencies required\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport re\nimport sys\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check if running on Kaggle or locally\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# Import competition metric only if not on Kaggle\nif not IS_KAGGLE:\n    from src.competition_metric import neurips_polymer_metric\n\n\n# Already checked above\n\n# Set paths based on environment\nif IS_KAGGLE:\n    # Kaggle competition paths\n    TRAIN_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/train.csv'\n    TEST_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/test.csv'\n    SUBMISSION_PATH = 'submission.csv'\n    \n    # Supplementary dataset paths\n    SUPP_PATHS = [\n        '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv',\n        '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv',\n        '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv',\n        '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv'\n    ]\nelse:\n    # Local paths\n    TRAIN_PATH = 'data/raw/train.csv'\n    TEST_PATH = 'data/raw/test.csv'\n    SUBMISSION_PATH = 'output/submission.csv'\n    \n    # Supplementary dataset paths\n    SUPP_PATHS = [\n        'data/raw/train_supplement/dataset1.csv',\n        'data/raw/train_supplement/dataset2.csv',\n        'data/raw/train_supplement/dataset3.csv',\n        'data/raw/train_supplement/dataset4.csv'\n    ]\n\ndef extract_molecular_features(smiles):\n    \"\"\"Extract features from SMILES string without external libraries\"\"\"\n    features = {}\n    \n    # Basic string features\n    features['length'] = len(smiles)\n    \n    # Count different atoms (case-sensitive for aromatic vs non-aromatic)\n    features['num_C'] = len(re.findall(r'C', smiles))\n    features['num_c'] = len(re.findall(r'c', smiles))  # aromatic carbon\n    features['num_O'] = len(re.findall(r'O', smiles))\n    features['num_o'] = len(re.findall(r'o', smiles))  # aromatic oxygen\n    features['num_N'] = len(re.findall(r'N', smiles))\n    features['num_n'] = len(re.findall(r'n', smiles))  # aromatic nitrogen\n    features['num_S'] = len(re.findall(r'S', smiles))\n    features['num_s'] = len(re.findall(r's', smiles))  # aromatic sulfur\n    features['num_F'] = smiles.count('F')\n    features['num_Cl'] = smiles.count('Cl')\n    features['num_Br'] = smiles.count('Br')\n    features['num_I'] = smiles.count('I')\n    features['num_P'] = smiles.count('P')\n    \n    # Count bonds\n    features['num_single_bonds'] = smiles.count('-')\n    features['num_double_bonds'] = smiles.count('=')\n    features['num_triple_bonds'] = smiles.count('#')\n    features['num_aromatic_bonds'] = smiles.count(':')\n    \n    # Count structural features\n    features['num_rings'] = sum(smiles.count(str(i)) for i in range(1, 10))\n    features['num_branches'] = smiles.count('(')\n    features['num_chiral_centers'] = smiles.count('@')\n    \n    # Polymer-specific features\n    features['has_polymer_end'] = int('*' in smiles)\n    # features['num_polymer_ends'] = smiles.count('*')  # Removed - may cause overfitting\n    \n    # Functional group patterns\n    features['has_carbonyl'] = int('C(=O)' in smiles or 'C=O' in smiles)\n    # features['has_hydroxyl'] = int('OH' in smiles or 'O[H]' in smiles)  # Removed - may cause overfitting\n    features['has_ether'] = int('COC' in smiles or 'cOc' in smiles)\n    features['has_amine'] = int('N' in smiles)\n    features['has_sulfone'] = int('S(=O)(=O)' in smiles)\n    features['has_ester'] = int('C(=O)O' in smiles or 'COO' in smiles)\n    features['has_amide'] = int('C(=O)N' in smiles or 'CON' in smiles)\n    \n    # Aromatic features\n    features['num_aromatic_atoms'] = features['num_c'] + features['num_n'] + features['num_o'] + features['num_s']\n    features['aromatic_ratio'] = features['num_aromatic_atoms'] / max(features['length'], 1)\n    \n    # Calculate derived features\n    features['heavy_atom_count'] = (features['num_C'] + features['num_c'] + \n                                   features['num_O'] + features['num_o'] + \n                                   features['num_N'] + features['num_n'] + \n                                   features['num_S'] + features['num_s'] + \n                                   features['num_F'] + features['num_Cl'] + \n                                   features['num_Br'] + features['num_I'] + \n                                   features['num_P'])\n    \n    features['heteroatom_count'] = (features['num_O'] + features['num_o'] + \n                                    features['num_N'] + features['num_n'] + \n                                    features['num_S'] + features['num_s'] + \n                                    features['num_F'] + features['num_Cl'] + \n                                    features['num_Br'] + features['num_I'] + \n                                    features['num_P'])\n    \n    features['heteroatom_ratio'] = features['heteroatom_count'] / max(features['heavy_atom_count'], 1)\n    \n    # Flexibility indicators\n    features['rotatable_bond_estimate'] = max(0, features['num_single_bonds'] - features['num_rings'])\n    features['flexibility_score'] = features['rotatable_bond_estimate'] / max(features['heavy_atom_count'], 1)\n    \n    # Size and complexity\n    features['molecular_complexity'] = (features['num_rings'] + features['num_branches'] + \n                                       features['num_chiral_centers'])\n    \n    # Additional polymer-specific patterns\n    features['has_phenyl'] = int('c1ccccc1' in smiles or 'c1ccc' in smiles)\n    features['has_cyclohexyl'] = int('C1CCCCC1' in smiles)\n    features['has_methyl'] = int(bool(re.search(r'C(?![a-zA-Z])', smiles)))\n    features['chain_length_estimate'] = max(len(x) for x in re.split(r'[\\(\\)]', smiles) if x)\n    \n    # Additional structural patterns\n    features['has_fused_rings'] = int(bool(re.search(r'[0-9].*c.*[0-9]', smiles)))\n    features['has_spiro'] = int('@' in smiles and smiles.count('@') > 1)\n    features['has_bridge'] = int(bool(re.search(r'[0-9].*[0-9]', smiles)))\n    \n    return features\n\ndef prepare_features(df):\n    \"\"\"Convert SMILES to molecular features\"\"\"\n    print(\"Extracting molecular features...\")\n    features_list = []\n    \n    for idx, smiles in enumerate(df['SMILES']):\n        if idx % 1000 == 0:\n            print(f\"Processing molecule {idx}/{len(df)}...\")\n        features = extract_molecular_features(smiles)\n        features_list.append(features)\n    \n    features_df = pd.DataFrame(features_list)\n    return features_df\n\n\ndef perform_cross_validation(X, y, cv_folds=5, target_columns=None):\n    \"\"\"\n    Perform cross-validation for separate models approach\n    \n    Args:\n        X: Features (numpy array or DataFrame)\n        y: Targets (DataFrame with multiple columns)\n        cv_folds: Number of cross-validation folds\n        target_columns: List of target column names\n    \n    Returns:\n        Dictionary with CV scores\n    \"\"\"\n    if IS_KAGGLE:\n        print(\"Cross-validation is not available on Kaggle\")\n        return None\n        \n    from sklearn.model_selection import KFold\n    \n    if target_columns is None:\n        target_columns = y.columns.tolist()\n    \n    print(f\"\\n=== Cross-Validation ({cv_folds} folds) ===\")\n    \n    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n    \n    # Store scores for each fold and target\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n        print(f\"\\nFold {fold + 1}/{cv_folds}...\")\n        \n        X_fold_train = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n        X_fold_val = X.iloc[val_idx] if hasattr(X, 'iloc') else X[val_idx]\n        y_fold_train = y.iloc[train_idx]\n        y_fold_val = y.iloc[val_idx]\n        \n        # Predictions for this fold\n        fold_predictions = np.zeros((len(val_idx), len(target_columns)))\n        \n        # Train separate models for each target\n        for i, target in enumerate(target_columns):\n            # Get non-missing samples for this target\n            mask = ~y_fold_train[target].isna()\n            \n            if mask.sum() > 0:\n                # Get samples with valid target values\n                mask_indices = np.where(mask)[0]\n                X_target = X_fold_train.iloc[mask_indices] if hasattr(X_fold_train, 'iloc') else X_fold_train[mask_indices]\n                y_target = y_fold_train[target].iloc[mask_indices]\n                \n                # Further filter to only keep rows with no missing features\n                if isinstance(X_target, pd.DataFrame):\n                    feature_complete_mask = ~X_target.isnull().any(axis=1)\n                else:\n                    # For numpy arrays\n                    feature_complete_mask = ~np.isnan(X_target).any(axis=1)\n                \n                X_target_complete = X_target[feature_complete_mask]\n                y_target_complete = y_target[feature_complete_mask]\n                \n                if len(X_target_complete) > 0:\n                    # Scale features (no imputation needed)\n                    scaler = StandardScaler()\n                    X_target_scaled = scaler.fit_transform(X_target_complete)\n                    \n                    # For validation set, we need to handle missing values\n                    imputer = SimpleImputer(strategy='mean')\n                    imputer.fit(X_target_complete)\n                    X_val_imputed = imputer.transform(X_fold_val)\n                    X_val_scaled = scaler.transform(X_val_imputed)\n                    \n                    # Use target-specific alpha\n                    target_alphas = {\n                        'Tg': 10.0,\n                        'FFV': 1.0,\n                        'Tc': 10.0,\n                        'Density': 5.0,\n                        'Rg': 10.0\n                    }\n                    alpha = target_alphas.get(target, 1.0)\n                    \n                    # Train Ridge model\n                    model = Ridge(alpha=alpha, random_state=42)\n                    model.fit(X_target_scaled, y_target_complete)\n                    \n                    # Predict on validation\n                    fold_predictions[:, i] = model.predict(X_val_scaled)\n                else:\n                    # No complete samples, use median\n                    fold_predictions[:, i] = y_fold_train[target].median()\n            else:\n                # No samples available, use median\n                fold_predictions[:, i] = y_fold_train[target].median()\n        \n        # Calculate fold score using competition metric\n        # Create predictions DataFrame with proper column names and index\n        fold_pred_df = pd.DataFrame(fold_predictions, columns=target_columns, index=y_fold_val.index)\n        \n        # Calculate competition metric\n        fold_score, individual_scores = neurips_polymer_metric(y_fold_val, fold_pred_df, target_columns)\n        \n        if not np.isnan(fold_score):\n            fold_scores.append(fold_score)\n            print(f\"  Fold {fold + 1} competition score: {fold_score:.4f}\")\n    \n    cv_mean = np.mean(fold_scores)\n    cv_std = np.std(fold_scores)\n    \n    print(f\"\\nCross-Validation Score (Competition Metric): {cv_mean:.4f} (+/- {cv_std:.4f})\")\n    \n    return {\n        'cv_mean': cv_mean,\n        'cv_std': cv_std,\n        'fold_scores': fold_scores\n    }\n\n\ndef main(cv_only=False):\n    \"\"\"\n    Main function to train model and make predictions\n    \n    Args:\n        cv_only: If True, only run cross-validation and skip submission generation\n    \"\"\"\n    print(\"=== Separate Ridge Models for Polymer Prediction ===\")\n    print(\"Loading training data...\")\n    \n    # Load main training data\n    train_df = pd.read_csv(TRAIN_PATH)\n    print(f\"Main training data shape: {train_df.shape}\")\n    \n    # Load and combine supplementary datasets\n    print(\"\\nLoading supplementary datasets...\")\n    all_train_dfs = [train_df]\n    \n    for supp_path in SUPP_PATHS:\n        try:\n            supp_df = pd.read_csv(supp_path)\n            print(f\"Loaded {supp_path}: {supp_df.shape}\")\n            all_train_dfs.append(supp_df)\n        except Exception as e:\n            print(f\"Could not load {supp_path}: {e}\")\n    \n    # Combine all training data\n    train_df = pd.concat(all_train_dfs, ignore_index=True)\n    print(f\"\\nCombined training data shape: {train_df.shape}\")\n    \n    # Load test data\n    print(\"\\nLoading test data...\")\n    test_df = pd.read_csv(TEST_PATH)\n    print(f\"Test data shape: {test_df.shape}\")\n    \n    # Extract features\n    print(\"\\nExtracting features from training data...\")\n    X_train = prepare_features(train_df)\n    \n    print(\"\\nExtracting features from test data...\")\n    X_test = prepare_features(test_df)\n    \n    # Prepare target variables\n    target_columns = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    y_train = train_df[target_columns]\n    \n    # Print feature statistics\n    print(f\"\\nFeature dimensions: {X_train.shape[1]} features\")\n    print(f\"Training samples: {X_train.shape[0]}\")\n    print(f\"Test samples: {X_test.shape[0]}\")\n    \n    # We'll handle missing values and scaling per target\n    print(\"\\nPreparing for target-specific training...\")\n    \n    # We don't need to handle missing target values since we train separate models\n    # Each model will only use samples with valid values for its specific target\n    \n    # Print target statistics\n    print(\"\\nTarget value statistics:\")\n    for col in target_columns:\n        print(f\"{col}: median={y_train[col].median():.4f}, \"\n              f\"missing={y_train[col].isna().sum()} ({y_train[col].isna().sum()/len(y_train)*100:.1f}%)\")\n    \n    # Run cross-validation if requested (but not on Kaggle)\n    if cv_only:\n        if IS_KAGGLE:\n            print(\"\\n⚠️  Cross-validation is not available in Kaggle notebooks\")\n            print(\"Proceeding with submission generation instead...\")\n        else:\n            cv_results = perform_cross_validation(X_train, y_train, cv_folds=5, target_columns=target_columns)\n            print(f\"\\n=== Cross-Validation Complete ===\")\n            return cv_results\n    \n    # Train separate Ridge regression models for each target\n    print(\"\\n=== Training Separate Models for Each Target ===\")\n    predictions = np.zeros((len(X_test), len(target_columns)))\n    \n    # Try different alpha values for each target\n    target_alphas = {\n        'Tg': 10.0,      # Higher regularization for sparse target\n        'FFV': 1.0,      # Lower regularization for dense target\n        'Tc': 10.0,      # Higher regularization for sparse target\n        'Density': 5.0,  # Medium regularization\n        'Rg': 10.0       # Higher regularization for sparse target\n    }\n    \n    for i, target in enumerate(target_columns):\n        print(f\"\\nTraining model for {target}...\")\n        \n        # Get non-missing samples for this target\n        mask = ~y_train[target].isna()\n        n_samples = mask.sum()\n        print(f\"  Available samples: {n_samples} ({n_samples/len(y_train)*100:.1f}%)\")\n        \n        if n_samples > 0:\n            # Get samples with valid target values\n            X_target = X_train[mask]\n            y_target = y_train[target][mask]\n            \n            # Further filter to only keep rows with no missing features\n            feature_complete_mask = ~X_target.isnull().any(axis=1)\n            X_target_complete = X_target[feature_complete_mask]\n            y_target_complete = y_target[feature_complete_mask]\n            \n            print(f\"  Complete samples (no missing features): {len(X_target_complete)} ({len(X_target_complete)/len(y_train)*100:.1f}%)\")\n            \n            if len(X_target_complete) > 0:\n                # Scale features (no imputation needed)\n                scaler = StandardScaler()\n                X_target_scaled = scaler.fit_transform(X_target_complete)\n                \n                # For test set, we need to handle missing values somehow\n                # Use mean imputation only for test set predictions\n                imputer = SimpleImputer(strategy='mean')\n                imputer.fit(X_target_complete)  # Fit on complete training data\n                X_test_imputed = imputer.transform(X_test)\n                X_test_scaled = scaler.transform(X_test_imputed)\n                \n                # Use target-specific alpha\n                alpha = target_alphas.get(target, 1.0)\n                print(f\"  Using alpha={alpha}\")\n                \n                # Train Ridge model for this target\n                model = Ridge(alpha=alpha, random_state=42)\n                model.fit(X_target_scaled, y_target_complete)\n                \n                # Make predictions\n                predictions[:, i] = model.predict(X_test_scaled)\n                print(f\"  Predictions: mean={predictions[:, i].mean():.4f}, std={predictions[:, i].std():.4f}\")\n            else:\n                # No complete samples available, use median\n                predictions[:, i] = y_train[target].median()\n                print(f\"  No complete samples available, using median: {predictions[:, i][0]:.4f}\")\n        else:\n            # Use median of available values if no samples\n            predictions[:, i] = y_train[target].median()\n            print(f\"  No samples available, using median: {predictions[:, i][0]:.4f}\")\n    \n    # Create submission DataFrame\n    submission_df = pd.DataFrame({\n        'id': test_df['id'],\n        'Tg': predictions[:, 0],\n        'FFV': predictions[:, 1],\n        'Tc': predictions[:, 2],\n        'Density': predictions[:, 3],\n        'Rg': predictions[:, 4]\n    })\n    \n    # Save submission\n    print(f\"\\nSaving submission to {SUBMISSION_PATH}...\")\n    submission_df.to_csv(SUBMISSION_PATH, index=False)\n    print(\"Done!\")\n    \n    # Display submission preview\n    print(\"\\nSubmission preview:\")\n    print(submission_df)\n    \n    print(\"\\n=== Model training complete! ===\")\n\nif __name__ == \"__main__\":\n    import sys\n    \n    # Check for command line arguments\n    cv_only = '--cv-only' in sys.argv or '--cv' in sys.argv\n    \n    main(cv_only=cv_only)","metadata":{"_uuid":"8cbe420d-bb89-41e2-818a-805411510d22","_cell_guid":"0684e733-e3ed-4f8d-9c94-97e6a1ac91e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-11T09:33:16.958127Z","iopub.execute_input":"2025-07-11T09:33:16.958550Z","iopub.status.idle":"2025-07-11T09:33:17.978378Z","shell.execute_reply.started":"2025-07-11T09:33:16.958524Z","shell.execute_reply":"2025-07-11T09:33:17.977165Z"}},"outputs":[{"name":"stdout","text":"=== Separate Ridge Models for Polymer Prediction ===\nLoading training data...\nMain training data shape: (7973, 7)\n\nLoading supplementary datasets...\nLoaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv: (874, 2)\nLoaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv: (7208, 1)\nLoaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv: (46, 2)\nLoaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv: (862, 2)\n\nCombined training data shape: (16963, 8)\n\nLoading test data...\nTest data shape: (3, 2)\n\nExtracting features from training data...\nExtracting molecular features...\nProcessing molecule 0/16963...\nProcessing molecule 1000/16963...\nProcessing molecule 2000/16963...\nProcessing molecule 3000/16963...\nProcessing molecule 4000/16963...\nProcessing molecule 5000/16963...\nProcessing molecule 6000/16963...\nProcessing molecule 7000/16963...\nProcessing molecule 8000/16963...\nProcessing molecule 9000/16963...\nProcessing molecule 10000/16963...\nProcessing molecule 11000/16963...\nProcessing molecule 12000/16963...\nProcessing molecule 13000/16963...\nProcessing molecule 14000/16963...\nProcessing molecule 15000/16963...\nProcessing molecule 16000/16963...\n\nExtracting features from test data...\nExtracting molecular features...\nProcessing molecule 0/3...\n\nFeature dimensions: 43 features\nTraining samples: 16963\nTest samples: 3\n\nPreparing for target-specific training...\n\nTarget value statistics:\nTg: median=77.8468, missing=16406 (96.7%)\nFFV: median=0.3640, missing=9071 (53.5%)\nTc: median=0.2360, missing=16226 (95.7%)\nDensity: median=0.9482, missing=16350 (96.4%)\nRg: median=15.0522, missing=16349 (96.4%)\n\n=== Training Separate Models for Each Target ===\n\nTraining model for Tg...\n  Available samples: 557 (3.3%)\n  Complete samples (no missing features): 557 (3.3%)\n  Using alpha=10.0\n  Predictions: mean=152.1101, std=20.8927\n\nTraining model for FFV...\n  Available samples: 7892 (46.5%)\n  Complete samples (no missing features): 7892 (46.5%)\n  Using alpha=1.0\n  Predictions: mean=0.3640, std=0.0083\n\nTraining model for Tc...\n  Available samples: 737 (4.3%)\n  Complete samples (no missing features): 737 (4.3%)\n  Using alpha=10.0\n  Predictions: mean=0.2366, std=0.0511\n\nTraining model for Density...\n  Available samples: 613 (3.6%)\n  Complete samples (no missing features): 613 (3.6%)\n  Using alpha=5.0\n  Predictions: mean=1.1384, std=0.0840\n\nTraining model for Rg...\n  Available samples: 614 (3.6%)\n  Complete samples (no missing features): 614 (3.6%)\n  Using alpha=10.0\n  Predictions: mean=19.7463, std=2.6284\n\nSaving submission to submission.csv...\nDone!\n\nSubmission preview:\n           id          Tg       FFV        Tc   Density         Rg\n0  1109053969  126.431277  0.357448  0.225903  1.241920  18.993507\n1  1422188626  152.292244  0.375793  0.179957  1.036092  16.970260\n2  2032016830  177.606743  0.358907  0.303791  1.137230  23.275171\n\n=== Model training complete! ===\n","output_type":"stream"}],"execution_count":11}]}